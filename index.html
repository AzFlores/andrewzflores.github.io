<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>My Research Portfolio</title>
  <style>
    /* --- Basic Reset & Fonts --- */
    * { box-sizing: border-box; margin: 0; padding: 0; }
    body { font-family: sans-serif; line-height: 1.6; color: #333; }
    a { color: #0066cc; text-decoration: none; }
    a:hover { text-decoration: underline; }

    /* --- Layout Containers --- */
    header, footer { text-align: center; padding: 2rem 1rem; background: #f8f8f8; }
    main { padding: 2rem 1rem; }
    .hidden { display: none; }

    /* --- Card Grid --- */
    .card-container {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
      gap: 1.5rem;
      max-width: 1000px;
      margin: 0 auto;
    }
    .card {
      background: white;
      border: 1px solid #ddd;
      border-radius: 4px;
      overflow: hidden;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
      cursor: pointer;
      transition: transform .1s;
    }
    .card:hover { transform: translateY(-4px); }
    .card img {
      width: 100%; height: 140px; object-fit: cover;
    }
    .card-content {
      padding: 1rem;
    }
    .card-content h2 {
      margin-bottom: .5rem;
      font-size: 1.25rem;
    }

    /* --- Blogpost Styling --- */
    .blogpost {
      max-width: 800px;
      margin: 2rem auto;
      padding: 0 1rem;
    }
    .blogpost img {
      max-width: 100%; margin: 1rem 0;
    }
    .blogpost h1 { margin-bottom: .5rem; }
    .blogpost h2 { margin-top: 1.5rem; margin-bottom: .5rem; }
    .blogpost p, .blogpost ul { margin-bottom: 1rem; }
    .blogpost ul { list-style: disc inside; }
    .subnav { padding: 1rem; background: #fafafa; }
    .subnav a { font-size: .9rem; }

    /* --- Footer --- */
    footer p { font-size: .9rem; }
  </style>
</head>
<body>

  <!-- LANDING -->
  <div id="landing">
    <header>
      <h1>My Research Portfolio</h1>
      <p>Exploring the dynamic integration of visual, linguistic, and contextual information in human meaning–making and word learning.</p>
    </header>

    <main>
      <div class="card-container">

        <!-- Project 1 -->
        <div class="card" onclick="showProject('proj1')">
          <img src="images/project1-thumb.jpg" alt="">
          <div class="card-content">
            <h2>Implicit Semantic Competition</h2>
            <p>How computational measures of visual and linguistic similarity predict cross‑modal priming and reveal dynamic semantic competition.</p>
          </div>
        </div>

        <!-- Project 2 -->
        <div class="card" onclick="showProject('proj2')">
          <img src="images/project2-thumb.jpg" alt="">
          <div class="card-content">
            <h2>Similarity Judgments</h2>
            <p>A dual‑manipulation study showing how task instructions and stimulus context flexibly weight visual, linguistic, and categorical cues in explicit similarity ratings.</p>
          </div>
        </div>

        <!-- Project 3 -->
        <div class="card" onclick="showProject('proj3')">
          <img src="images/project3-thumb.jpg" alt="">
          <div class="card-content">
            <h2>Distributional Vocabulary Model</h2>
            <p>Introducing Pro‑KWo: how the proportion of known‑word co‑occurrences in child‑directed speech drives word acquisition trajectories.</p>
          </div>
        </div>

      </div>
    </main>

    <footer>
      <p>© 2025 Andrew Z. Flores • <a href="mailto:azf2@illinois.edu">azf2@illinois.edu</a></p>
    </footer>
  </div>

  <!-- PROJECT 1 -->
  <article id="proj1" class="blogpost hidden">
    <nav class="subnav">
      <a href="#" onclick="showLanding()">← Back to Portfolio</a>
    </nav>
    <h1>Implicit Semantic Competition</h1>
    <p><em>Andrew Z. Flores, Jessica L. Montag, & Jon A. Willits</em></p>
    <img src="images/project1-header.jpg" alt="Cross-modal priming example">

    <h2>Introduction</h2>
    <p>
      We investigated the process by which visual and linguistic similarity jointly drive speeded word recognition. By pre‑computing ResNet‑derived visual distances and Word2Vec‑derived linguistic distances for 128 object concepts at four categorical relatedness levels, we ran a cross‑modal priming study (image → word) to see how these metrics predict reaction times.
    </p>

    <h2>Key Findings</h2>
    <ul>
      <li><strong>Linguistic similarity</strong> (Word2Vec) was the strongest single predictor of priming magnitude.</li>
      <li><strong>Visual metrics</strong> (ResNet L0, L7, SSIM) contributed uniquely under perceptually focused tasks.</li>
      <li>Evidence supports a <strong>dynamic integration</strong>: semantic competition flexibly recruits different features based on task demands.</li>
    </ul>

    <h2>Methods & Analysis</h2>
    <p>
      Reaction time data (N=240) were analyzed with linear mixed‑effects models including both similarity metrics and their interactions with relatedness level. Computational models recapitulated the behavioral patterns, underscoring the power of combined visual‑linguistic feature spaces.
    </p>

    <h2>Implications</h2>
    <p>
      These results highlight how the mind rapidly integrates multifaceted semantic cues—even when not explicitly instructed to do so—offering a unified account of implicit competition in spoken‐word recognition.
    </p>
  </article>

  <!-- PROJECT 2 -->
  <article id="proj2" class="blogpost hidden">
    <nav class="subnav">
      <a href="#" onclick="showLanding()">← Back to Portfolio</a>
    </nav>
    <h1>Similarity Judgments</h1>
    <p><em>Andrew Z. Flores, Jessica L. Montag, & Jon A. Willits</em></p>
    <img src="images/project2-header.jpg" alt="Similarity judgment task">

    <h2>Introduction</h2>
    <p>
      Explicit similarity ratings (“How similar are these two items?”) are a staple of cognitive science, yet the underlying feature recruitment remains unclear. Are judgments driven by stable stored features or by dynamic, context‑sensitive comparisons?
    </p>

    <h2>Design</h2>
    <ul>
      <li><strong>Instruction manipulation</strong>: Broad semantic vs. purely visual focus.</li>
      <li><strong>Context manipulation</strong>: Within (full range of relatedness levels) vs. Between (single level only).</li>
    </ul>
    <p>
      Participants (N=480) rated 128 image‑pairs on a 0–5 Likert slider. We computed both mean ratings and ran mixed‑effects regressions, then linked behavior to ResNet (visual) and Word2Vec (linguistic) metrics.
    </p>

    <h2>Behavioral Results</h2>
    <ul>
      <li>Strong positive correlations between ratings and categorical relatedness—steeper in the Within context.</li>
      <li>Instructions alone did not fully override categorical structure, even under visual focus.</li>
      <li>Stimulus context exerted a scaling effect: Within-context participants used the scale more conservatively.</li>
    </ul>

    <h2>Computational Modeling</h2>
    <p>
      Progressive mixed‑effects models showed that high‑level linguistic (Word2Vec) and visual (ResNet L7) metrics drove similarity judgments most in the Within context, whereas low‑level visual cues gained traction under Between/Visual conditions.
    </p>

    <h2>Discussion & Significance</h2>
    <p>
      These findings support a constructivist view: similarity is not a fixed property but an active, context‑dependent process that flexibly weights diverse information sources in line with task demands and stimulus distribution.
    </p>
  </article>

  <!-- PROJECT 3 -->
  <article id="proj3" class="blogpost hidden">
    <nav class="subnav">
      <a href="#" onclick="showLanding()">← Back to Portfolio</a>
    </nav>
    <h1>Using Known Words to Learn More Words</h1>
    <p><em>Andrew Z. Flores, Jessica L. Montag, & Jon A. Willits</em></p>
    <img src="images/project3-header.jpg" alt="Child vocabulary acquisition">

    <h2>Abstract</h2>
    <p>
      We introduce <strong>Pro‑KWo</strong>, a distributional predictor of word acquisition that leverages children’s prior knowledge. Pro‑KWo quantifies the proportion of a word’s occurrences in child‑directed speech that co‑occur with already‐known words. Across ages 16–30 months, Pro‑KWo outperforms frequency, lexical diversity, and document diversity at predicting production on the MCDI, and—unlike those measures—remains robust across grammatical classes.
    </p>

    <h2>Key Insights</h2>
    <ul>
      <li><strong>Quality over Quantity</strong>: Beyond mere frequency, Pro‑KWo indexes high‑quality learning episodes where known words bootstrap new ones.</li>
      <li><strong>Prior Knowledge Matters</strong>: Words surrounded by familiar vocabulary are learned earlier, mirroring behavioral bootstrapping and mutual exclusivity effects.</li>
      <li><strong>Grammatical‑Class Independence</strong>: Pro‑KWo predicts nouns, verbs, adjectives, and function words equally well—no separate mechanisms needed.</li>
    </ul>

    <h2>Method Snapshot</h2>
    <p>
      Using 49 American‑English CHILDES corpora, we extracted co‑occurrence counts within a 7‑word window for the 500 MCDI production items. At each month (16–30 m), we weighted each co‑occurrence by the proportion of children reported to know the co‑occurring word (MCDIp), yielding Pro‑KWo = (sum of known‑word co‑occurrences)/(sum of all co‑occurrences).
    </p>

    <h2>Results</h2>
    <ul>
      <li>Pro‑KWo ↔ MCDIp correlations of .60–.75 across ages—far above frequency (.20–.40) or diversity metrics (ns).</li>
      <li>Mixed‑effects logistic models: a 1 SD rise in Pro‑KWo increased production odds by ~0.99, versus ~0.56 for frequency.</li>
      <li>Shuffled MCDIp control showed Pro‑KWo’s power is not an artifact of circularity.</li>
    </ul>

    <h2>Conclusions</h2>
    <p>
      Pro‑KWo bridges behavioral insights and corpus methods: the contexts that allow children to leverage existing vocabulary are the ones that foster new word learning. This single, class‑agnostic metric opens a path toward richer, behaviorally grounded statistical models of language development.
    </p>
  </article>

  <script>
    function showProject(id) {
      document.getElementById('landing').classList.add('hidden');
      ['proj1','proj2','proj3'].forEach(pid => 
        document.getElementById(pid).classList.add('hidden')
      );
      document.getElementById(id).classList.remove('hidden');
    }
    function showLanding() {
      document.getElementById('landing').classList.remove('hidden');
      ['proj1','proj2','proj3'].forEach(pid => 
        document.getElementById(pid).classList.add('hidden')
      );
      window.scrollTo(0,0);
    }
  </script>
</body>
</html>
